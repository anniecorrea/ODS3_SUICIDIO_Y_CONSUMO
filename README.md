# üè• ODS3: An√°lisis de Suicidio y Consumo de Sustancias Psicoactivas en Bogot√°

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.5.1-017CEE?logo=apache-airflow)](https://airflow.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![MySQL](https://img.shields.io/badge/MySQL-8.0-4479A1?logo=mysql&logoColor=white)](https://www.mysql.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)

## üìã Descripci√≥n del Proyecto

Este proyecto implementa un pipeline ETL (Extract, Transform, Load) automatizado con Apache Airflow para analizar la relaci√≥n entre el consumo de sustancias psicoactivas (SPA) y la conducta suicida en Bogot√°, Colombia. Los datos se procesan, transforman y cargan en un modelo dimensional (star schema) en MySQL para facilitar el an√°lisis y generaci√≥n de insights.

### üéØ Objetivo

Contribuir al **Objetivo de Desarrollo Sostenible 3 (ODS 3)** - Salud y Bienestar, mediante el an√°lisis de datos abiertos para identificar patrones y factores de riesgo asociados al consumo de sustancias y conductas suicidas en diferentes poblaciones de Bogot√°.

## üèóÔ∏è Arquitectura del Proyecto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CAPA DE EXTRACCI√ìN                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üì° API Datos Abiertos Bogot√°  ‚îÇ  üìÑ CSV Local               ‚îÇ
‚îÇ  (Conducta Suicida)            ‚îÇ  (Consumo SPA)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ                                  ‚îÇ
               ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   CAPA DE TRANSFORMACI√ìN                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üîÑ Limpieza y estandarizaci√≥n de datos                      ‚îÇ
‚îÇ  üîÑ Formato Tidy Data                                        ‚îÇ
‚îÇ  üîÑ Control de calidad (QC)                                  ‚îÇ
‚îÇ  üîÑ Merge de datasets                                        ‚îÇ
‚îÇ  üîÑ C√°lculo de porcentajes y agregaciones                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ
               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CAPA DE CARGA                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  üóÑÔ∏è MySQL - Modelo Dimensional (Star Schema)                ‚îÇ
‚îÇ     ‚Ä¢ dim_tiempo (a√±o)                                       ‚îÇ
‚îÇ     ‚Ä¢ dim_ubicacion (UPZ)                                    ‚îÇ
‚îÇ     ‚Ä¢ dim_perfil_demografico (sexo, edad, educaci√≥n)         ‚îÇ
‚îÇ     ‚Ä¢ dim_clasificacion (tipo de evento)                     ‚îÇ
‚îÇ     ‚Ä¢ fact_casos (m√©tricas y agregaciones)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Caracter√≠sticas Principales

- ‚úÖ **Pipeline ETL Automatizado**: Orquestado con Apache Airflow
- ‚úÖ **Extracci√≥n Multi-fuente**: API REST y archivos CSV
- ‚úÖ **Transformaci√≥n Robusta**: Limpieza, validaci√≥n y formato tidy
- ‚úÖ **Control de Calidad**: Reportes JSON con m√©tricas de calidad
- ‚úÖ **Modelo Dimensional**: Star schema optimizado para an√°lisis
- ‚úÖ **Dockerizado**: F√°cil despliegue con Docker Compose
- ‚úÖ **Escalable**: Procesamiento por lotes y optimizado
- ‚úÖ **Logging Completo**: Trazabilidad de todo el proceso

## üõ†Ô∏è Tecnolog√≠as Utilizadas

| Tecnolog√≠a | Versi√≥n | Prop√≥sito |
|------------|---------|-----------|
| **Apache Airflow** | 2.5.1 | Orquestaci√≥n del pipeline ETL |
| **Python** | 3.8+ | Lenguaje de programaci√≥n principal |
| **Pandas** | 2.3.3 | Procesamiento y an√°lisis de datos |
| **MySQL** | 8.0 | Base de datos relacional |
| **Docker** | Latest | Contenedorizaci√≥n de servicios |
| **PostgreSQL** | 13 | Metadata de Airflow |
| **Redis** | Latest | Message broker para Celery |

## üìä Modelo de Datos

### Dimensiones

#### üìÖ `dim_tiempo`
- **id_tiempo** (PK)
- anio

#### üìç `dim_ubicacion`
- **id_ubicacion** (PK)
- upz (Unidad de Planeamiento Zonal)

#### üë§ `dim_perfil_demografico`
- **id_perfil** (PK)
- sexo
- ciclo_vida
- nivel_educativo

#### üè∑Ô∏è `dim_clasificacion`
- **id_clasificacion** (PK)
- clasificacion (tipo de evento: intento, amenaza, etc.)

### Tabla de Hechos

#### üìà `fact_casos`
- **id_fact** (PK)
- id_tiempo (FK)
- id_ubicacion (FK)
- id_perfil (FK)
- id_clasificacion (FK)
- **M√©tricas de casos:**
  - casos_spa, casos_sui
- **Sitios de consumo:**
  - sitio_vivienda, sitio_parque, sitio_est_educativo, sitio_bares_tabernas, sitio_via_publica, sitio_casa_amigos
  - pct_sitio_* (porcentajes)
- **Factores de riesgo:**
  - enfermedades_dolorosas, maltrato_sexual, muerte_familiar, conflicto_pareja, problemas_economicos, esc_educ, problemas_juridicos, problemas_laborales, suicidio_amigo
  - pct_* (porcentajes)

## üîÑ Pipeline ETL

### Flujo de Tareas

```mermaid
graph TD
    A[extract_conducta_suicida] --> C[transform_conducta_suicida]
    B[extract_consumo_sustancias] --> D[transform_consumo_spa]
    C --> E[merge_datasets]
    D --> E
    E --> F[load_to_mysql]
```

### Descripci√≥n de Tareas

1. **extract_conducta_suicida** üì°
   - Extrae datos de la API de Datos Abiertos Bogot√°
   - Endpoint: https://datosabiertos.bogota.gov.co
   - Paginaci√≥n autom√°tica (32,000 registros por p√°gina)
   - Guarda CSV en `notebooks/data/conductasuicida.csv`

2. **extract_consumo_sustancias** üìÑ
   - Lee archivo CSV local de consumo de SPA
   - Ruta: `notebooks/data/consumosustancias.csv`

3. **transform_conducta_suicida** üîÑ
   - Limpieza y estandarizaci√≥n de datos
   - Separaci√≥n de clasificaciones m√∫ltiples
   - Formato tidy data
   - Genera reporte QC: `data_out/suicida_qc.json`
   - Salida: `data_out/suicida_tidy.csv`

4. **transform_consumo_spa** üîÑ
   - Normalizaci√≥n de campos
   - Transformaci√≥n a formato tidy
   - Genera reporte QC: `data_out/spa_qc.json`
   - Salida: `data_out/spa_tidy.csv`

5. **merge_datasets** üîó
   - Combina datasets transformados
   - Join tipo: INNER (por a√±o, UPZ, sexo, ciclo_vida, nivel_educativo)
   - Calcula porcentajes de todas las variables
   - Genera reporte QC: `data_out/merge_qc.json`
   - Salida: `data_out/merged_spa_suicidas.csv`

6. **load_to_mysql** üóÑÔ∏è
   - Crea base de datos si no existe
   - Crea modelo dimensional (tablas dim_* y fact_casos)
   - Carga incremental por lotes (1000 registros)
   - Usa cach√©s para optimizar inserciones en dimensiones

## üö¶ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Docker Desktop instalado
- Docker Compose instalado
- Al menos 4 GB de RAM disponible
- 10 GB de espacio en disco

### Pasos de Instalaci√≥n

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/anniecorrea/ODS3_SUICIDIO_Y_CONSUMO.git
   cd ODS3_SUICIDIO_Y_CONSUMO
   ```

2. **Configurar variables de entorno (opcional)**
   
   Crear archivo `.env` en la ra√≠z:
   ```env
   AIRFLOW_UID=50000
   MYSQL_HOST=mysql
   MYSQL_DATABASE=ODS3_SPA_SUICIDAS
   MYSQL_USER=root
   MYSQL_PASSWORD=airflow
   MYSQL_PORT=3306
   ```

3. **Inicializar Airflow**
   ```bash
   docker-compose up airflow-init
   ```

4. **Levantar todos los servicios**
   ```bash
   docker-compose up -d
   ```

5. **Verificar estado de servicios**
   ```bash
   docker-compose ps
   ```

### Acceso a Servicios

| Servicio | URL | Credenciales |
|----------|-----|--------------|
| **Airflow UI** | http://localhost:8080 | Usuario: `airflow`<br>Contrase√±a: `airflow` |
| **MySQL** | localhost:3307 | Usuario: `root`<br>Contrase√±a: `airflow` |

## üìñ Uso del Sistema

### Activar el DAG

1. Acceder a Airflow UI: http://localhost:8080
2. Buscar el DAG: `ods3_etl_suicidio_consumo`
3. Activar el toggle para habilitar el DAG
4. El pipeline se ejecutar√° diariamente de forma autom√°tica

### Ejecuci√≥n Manual

Para ejecutar el pipeline manualmente:

1. En Airflow UI, hacer clic en el DAG
2. Clic en "Trigger DAG" (bot√≥n de play ‚ñ∂Ô∏è)
3. Confirmar la ejecuci√≥n

## üß™ Testing y Validaci√≥n

### Ejecutar Tests
```bash
# Entrar al contenedor de Airflow
docker-compose exec airflow-worker bash

# Ejecutar tests unitarios (si existen)
pytest tests/
```

### Validar Carga de Datos
```bash
# Conectar a MySQL
docker-compose exec mysql mysql -u root -pairflow ODS3_SPA_SUICIDAS

# Verificar carga
SELECT COUNT(*) FROM fact_casos;
SELECT COUNT(*) FROM dim_tiempo;
SELECT COUNT(*) FROM dim_ubicacion;
SELECT COUNT(*) FROM dim_perfil_demografico;
SELECT COUNT(*) FROM dim_clasificacion;
```

### Problema: Servicios no inician
```bash
# Verificar logs
docker-compose logs airflow-webserver
docker-compose logs mysql

# Reiniciar servicios
docker-compose down
docker-compose up -d
```

### Problema: DAG no aparece en UI
```bash
# Verificar sintaxis del DAG
docker-compose exec airflow-scheduler python /opt/airflow/dags/dag_etl.py

# Refrescar DAGs
docker-compose restart airflow-scheduler
```

### Problema: Error de conexi√≥n a MySQL
- Verificar que el servicio MySQL est√© corriendo: `docker-compose ps`
- Revisar credenciales en variables de entorno
- Esperar a que MySQL complete su inicializaci√≥n (~30 segundos)